{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0bf88d",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98510cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 12:11:26.703752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6257821b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 11:44:54.382717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:44:58.612904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:44:58.614173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:44:58.654852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 11:44:58.656289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:44:58.658709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:44:58.660056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:45:13.558511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:45:13.558825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:45:13.559055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 11:45:13.559289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2707 MB memory:  -> device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c86906e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190f133",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf51da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Conv3D, MaxPooling2D, AveragePooling2D, Flatten, Dense, LSTM, TimeDistributed, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# import keras.utils as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9483893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ff3b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa0edb",
   "metadata": {},
   "source": [
    "# Proprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8412b6e3",
   "metadata": {},
   "source": [
    "## Set Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd172196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "DATA_DIR = 'Validate'\n",
    "DATA2_DIR = 'Validate2'\n",
    "CATEGORIES = ['dining', 'drawing', 'mopping_floor', 'reading_book', 'running_on_treadmill','sleeping','watching_tv']\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "INPUT_SHAPE = (40, 112, 112, 3)  # (frames, height, width, channels)\n",
    "BATCH_SIZE = 5\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a721d",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c4c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(category):\n",
    "    X = []\n",
    "    y = []\n",
    "    exampleCount = 1\n",
    "    category_dir = os.path.join(DATA2_DIR, category)\n",
    "    print(\"Loading \", category)\n",
    "        \n",
    "    for video in os.listdir(category_dir):\n",
    "        video_dir = os.path.join(category_dir, video)\n",
    "\n",
    "        if len(os.listdir(video_dir)) > 0:\n",
    "            print(\"Loading video \",str(exampleCount), ': ', str(video))  \n",
    "            frames = []\n",
    "\n",
    "            for frame_file in range(40):\n",
    "                img_path = os.path.join(video_dir, str(str(frame_file) + '.jpg'))\n",
    "                img = image.load_img(img_path, target_size=(112, 112))\n",
    "                x = image.img_to_array(img)\n",
    "                x = x/255\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                frames.append(x)\n",
    "            X.append(np.concatenate(frames))\n",
    "            y.append(category)\n",
    "            exampleCount += 1\n",
    "            frames.clear()\n",
    "            gc.collect()\n",
    "\n",
    "            for frame_file in range(40,80):\n",
    "                img_path = os.path.join(video_dir, str(str(frame_file) + '.jpg'))\n",
    "                img = image.load_img(img_path, target_size=(112, 112))\n",
    "                x = image.img_to_array(img)\n",
    "                x = x/255\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                frames.append(x)\n",
    "            X.append(np.concatenate(frames))\n",
    "            y.append(category)\n",
    "            exampleCount += 1\n",
    "            gc.collect()\n",
    "\n",
    "        else:\n",
    "            print('Skip vid: ', video)    \n",
    "                \n",
    "    print(\"Finished loading \", category)\n",
    "    print(\"Formating input X\")\n",
    "    X = np.array(X)\n",
    "    print(\"Formating label y\")\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print('\\nSummary: \\n')\n",
    "    print(str(category), ': ', str(exampleCount))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ddda473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(X, model, step):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while end < X.shape[0]:\n",
    "        end = start + step\n",
    "        if start != 0:\n",
    "            print('Processing video ', start, ' - ', end)\n",
    "            X_sliced = X[start:end,:,:,:,:]\n",
    "            X_train = model.predict_on_batch(X_sliced)\n",
    "            X_train_concat = np.concatenate((X_train_concat,X_train))\n",
    "            start = end\n",
    "            gc.collect()\n",
    "        else:      \n",
    "            print('Start processing...')\n",
    "            print('Processing video ', start, ' - ', end)\n",
    "            X_sliced = X[start:end,:,:,:,:]\n",
    "            X_train = model.predict_on_batch(X_sliced)\n",
    "            X_train_concat = X_train\n",
    "            start = end\n",
    "            gc.collect()\n",
    "\n",
    "    print('Processed ', end, ' video')\n",
    "    \n",
    "    return X_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae85f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(categories, X, model):\n",
    "    if len(X) != 0:\n",
    "        x_processed = np.load('/home/jeheng/Desktop/numpy/Validate_data_processed/'+ model + '/' + categories + '_processed.npy')\n",
    "        X = np.concatenate((X, x_processed))\n",
    "        return X, x_processed.shape\n",
    "    else:\n",
    "        X = np.load('/home/jeheng/Desktop/numpy/Validate_data_processed/'+ model + '/' + categories + '_processed.npy')\n",
    "        return X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f23c68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_label(categories, y, model):\n",
    "    if len(y) != 0:\n",
    "        y_processed = np.load('/home/jeheng/Desktop/numpy/Validate_data_processed/'+ model + '/' + categories + '_label.npy')\n",
    "        y = np.concatenate((y, y_processed))\n",
    "        return y, y_processed.shape\n",
    "    else:\n",
    "        y = np.load('/home/jeheng/Desktop/numpy/Validate_data_processed/'+ model + '/' + categories + '_label.npy')\n",
    "        return y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc41f6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6779042c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_load_data(categories, X_train, X_test):\n",
    "    if len(X_train) != 0 and len(X_test) != 0:\n",
    "        x_processed = np.load('f:/Kinetic_700/numpy2/' + categories + '_processed.npy')\n",
    "        split = round(len(x_processed)*0.8)\n",
    "        X_train = np.concatenate((X, x_processed[0:split]))\n",
    "        X_test = np.concatenate((X, x_processed[split:len(x_processed)]))\n",
    "        return X_train, X_test, x_processed.shape, split\n",
    "    else:\n",
    "        x_processed = np.load('f:/Kinetic_700/numpy2/' + categories + '_processed.npy')\n",
    "        split = round(len(x_processed)*0.8)\n",
    "        X_train = x_processed[0:split]\n",
    "        X_test = x_processed[split:len(x_processed)]\n",
    "        return X_train, X_test, x_processed.shape, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_load_label(categories, y_train, y_test):\n",
    "    if len(y_train) != 0 and len(y_test) != 0:\n",
    "        \n",
    "        y_processed = np.load('f:/Kinetic_700/numpy2/' + categories + '_label.npy')\n",
    "        split = round(len(y_processed)*0.8)\n",
    "        \n",
    "        y_train = np.concatenate((y_train, y_processed[0:split]))\n",
    "        y_test = np.concatenate((y_train, y_processed[split:len(y_processed)]))\n",
    "        \n",
    "        return y_train, y_test, y_processed.shape, split\n",
    "    else:\n",
    "        y_processed = np.load('f:/Kinetic_700/numpy2/' + categories + '_label.npy')\n",
    "        split = round(len(y_processed)*0.8)\n",
    "\n",
    "        y_train = y_processed[0:split]\n",
    "        y_test = y_processed[split:len(y_processed)]\n",
    "        return y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509525b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y[y == 'dining'] = 0\n",
    "    y[y == 'drawing'] = 1\n",
    "    y[y == 'mopping_floor'] = 2\n",
    "    y[y == 'reading_book'] = 3\n",
    "    y[y == 'running_on_treadmill'] = 4\n",
    "    y[y == 'sleeping'] = 5\n",
    "    y[y == 'watching_tv'] = 6\n",
    "    \n",
    "    y_encode = to_categorical(y, num_classes=7)\n",
    "\n",
    "    print(\"Shape of y_encode: \", y_encode.shape)\n",
    "    \n",
    "    return y_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b99311",
   "metadata": {},
   "source": [
    "## Define extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "521451fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the base model of pre-trained VGG16 model\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1476cc4",
   "metadata": {},
   "source": [
    "### ============================ VGG16 GlobalAVG  ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vgg16 model\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "# Freeze vgg16 layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Extract features using vgg16\n",
    "inputs = Input(shape=INPUT_SHAPE)\n",
    "l1 = TimeDistributed(vgg16)(inputs)\n",
    "l2 = TimeDistributed(GlobalAveragePooling2D())(l1)\n",
    "#outputs = TimeDistributed(Flatten())(l2)\n",
    "\n",
    "# Create model\n",
    "vgg16_gavg = Model(inputs, l2)\n",
    "vgg16_gavg.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f777e",
   "metadata": {},
   "source": [
    "### ============================ VGG16 AVG ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "578e689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 40, 112, 112, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 40, 3, 3, 512)    14714688  \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 40, 2, 2, 512)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 40, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load vgg16 model\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "# Freeze vgg16 layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Extract features using vgg16\n",
    "inputs = Input(shape=INPUT_SHAPE)\n",
    "l1 = TimeDistributed(vgg16)(inputs)\n",
    "l2 = TimeDistributed(AveragePooling2D(pool_size=(2,2),strides=1))(l1)\n",
    "outputs = TimeDistributed(Flatten())(l2)\n",
    "\n",
    "# Create model\n",
    "vgg16_avg = Model(inputs, outputs)\n",
    "vgg16_avg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f89728",
   "metadata": {},
   "source": [
    "### ============================ Resnet GlobalAVG ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0ed7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 40, 112, 112, 3)  0         \n",
      "                             ]                                   \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 40, 4, 4, 2048)   23587712  \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 40, 2048)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load ResNet50 model\n",
    "resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "# Freeze ResNet50 layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Extract features using ResNet50\n",
    "inputs = Input(shape=INPUT_SHAPE)\n",
    "l1 = TimeDistributed(resnet)(inputs)\n",
    "l2 = TimeDistributed(GlobalAveragePooling2D())(l1)\n",
    "#outputs = TimeDistributed(Flatten())(l2)\n",
    "\n",
    "# Create model\n",
    "resnet_gavg = Model(inputs, l2)\n",
    "resnet_gavg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d6e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac7ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aebb7dd",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67383c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class -- ['dining', 'drawing', 'mopping_floor', 'reading_book', 'running_on_treadmill','sleeping','watching_tv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e892db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'watching_tv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "58d33fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  watching_tv\n",
      "Loading video  1 :  illbIdQ0W0g_000002_000012\n",
      "Loading video  3 :  iMLgP40MaMs_000017_000027\n",
      "Loading video  5 :  ImvV7a00Iso_000036_000046\n",
      "Loading video  7 :  ItfZ6b8nduo_000012_000022\n",
      "Loading video  9 :  IX864cuGCXg_000008_000018\n",
      "Loading video  11 :  i_CNmgtgpvg_000033_000043\n",
      "Loading video  13 :  I_QxQbSyPGA_000040_000050\n",
      "Loading video  15 :  j8ilCoKhaxU_000003_000013\n",
      "Loading video  17 :  JHZrmSX7Sko_000005_000015\n",
      "Loading video  19 :  jlDKyHzuwd4_000123_000133\n",
      "Loading video  21 :  JYBOtrMTahc_000039_000049\n",
      "Loading video  23 :  Jz5qdEgpmA4_000005_000015\n",
      "Loading video  25 :  JzOx6KiV1f8_000000_000010\n",
      "Loading video  27 :  k659P60d5TM_000013_000023\n",
      "Loading video  29 :  Kh6wv2EMqbo_000000_000010\n",
      "Loading video  31 :  KRsGr1Kq9ak_000063_000073\n",
      "Loading video  33 :  KRzV6wDS000_000002_000012\n",
      "Loading video  35 :  K_QuvGRl7PQ_000000_000010\n",
      "Loading video  37 :  lhb4IAWSxKE_000027_000037\n",
      "Loading video  39 :  L_dzgn4x5KY_000010_000020\n",
      "Loading video  41 :  mbD5loJ8FH8_000069_000079\n",
      "Loading video  43 :  mdyRk_rOAhQ_000045_000055\n",
      "Loading video  45 :  mf-9v1gNFk4_000000_000010\n",
      "Loading video  47 :  Mj9RLcxU4aw_000113_000123\n",
      "Loading video  49 :  mRvfVWZEfug_000010_000020\n",
      "Loading video  51 :  mw8-5iUdaxc_000001_000011\n",
      "Loading video  53 :  mWIDgYEkf_0_000033_000043\n",
      "Loading video  55 :  N9tkgaJDW2A_000001_000011\n",
      "Loading video  57 :  NMcMDUiSYNA_000042_000052\n",
      "Loading video  59 :  NN6LeXCyrP0_000041_000051\n",
      "Loading video  61 :  NO9pcpJZs8M_000080_000090\n",
      "Loading video  63 :  nTaVteKpxp0_000003_000013\n",
      "Loading video  65 :  nuNQd7x2JhI_000004_000014\n",
      "Loading video  67 :  o25WH5N9Tfk_000002_000012\n",
      "Loading video  69 :  o6QzPBSA4KY_000019_000029\n",
      "Loading video  71 :  O6uz2rS6Qts_000000_000010\n",
      "Loading video  73 :  OF6q2MEw08c_000028_000038\n",
      "Loading video  75 :  ojl1sPxpB2A_000014_000024\n",
      "Loading video  77 :  OLymtVJko_M_000243_000253\n",
      "Loading video  79 :  otgEWqm1yVc_000032_000042\n",
      "Loading video  81 :  oyGlSsi-g-A_000001_000011\n",
      "Loading video  83 :  Oz9aIPG6H28_000008_000018\n",
      "Loading video  85 :  P-Kp2d3H9UU_000092_000102\n",
      "Loading video  87 :  p8kxYjevOV0_000027_000037\n",
      "Loading video  89 :  Pc5i41FZGbw_000264_000274\n",
      "Loading video  91 :  Pc91PKd-Nvs_000000_000010\n",
      "Loading video  93 :  pGCFm6pjIAA_000021_000031\n",
      "Loading video  95 :  pMmecxzkBtQ_000033_000043\n",
      "Loading video  97 :  PTfjT4aVw6E_000039_000049\n",
      "Finished loading  watching_tv\n",
      "Formating input X\n",
      "Formating label y\n",
      "\n",
      "Summary: \n",
      "\n",
      "watching_tv :  99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = preprocess_data(category)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0b463acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (98, 40, 112, 112, 3)\n",
      "Shape of y:  (98,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2b92c11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590069928"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "aff66533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "e49f4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(category + '_full_validate', X)\n",
    "np.save(category + '_full_label_validate', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906ebb4",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6005adf8",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cc4b10",
   "metadata": {},
   "source": [
    "## Extract Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246625bd",
   "metadata": {},
   "source": [
    "### VGG16_AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2e157625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n",
      "Processing video  0  -  4\n",
      "Processing video  4  -  8\n",
      "Processing video  8  -  12\n",
      "Processing video  12  -  16\n",
      "Processing video  16  -  20\n",
      "Processing video  20  -  24\n",
      "Processing video  24  -  28\n",
      "Processing video  28  -  32\n",
      "Processing video  32  -  36\n",
      "Processing video  36  -  40\n",
      "Processing video  40  -  44\n",
      "Processing video  44  -  48\n",
      "Processing video  48  -  52\n",
      "Processing video  52  -  56\n",
      "Processing video  56  -  60\n",
      "Processing video  60  -  64\n",
      "Processing video  64  -  68\n",
      "Processing video  68  -  72\n",
      "Processing video  72  -  76\n",
      "Processing video  76  -  80\n",
      "Processing video  80  -  84\n",
      "Processing video  84  -  88\n",
      "Processing video  88  -  92\n",
      "Processing video  92  -  96\n",
      "Processing video  96  -  100\n",
      "Processed  100  video\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'vgg16_avg'\n",
    "X_processed = extract_feature(X, vgg16_avg, 4)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a9bf2",
   "metadata": {},
   "source": [
    "### Resnet_GAVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e844b7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n",
      "Processing video  0  -  4\n",
      "Processing video  4  -  8\n",
      "Processing video  8  -  12\n",
      "Processing video  12  -  16\n",
      "Processing video  16  -  20\n",
      "Processing video  20  -  24\n",
      "Processing video  24  -  28\n",
      "Processing video  28  -  32\n",
      "Processing video  32  -  36\n",
      "Processing video  36  -  40\n",
      "Processing video  40  -  44\n",
      "Processing video  44  -  48\n",
      "Processing video  48  -  52\n",
      "Processing video  52  -  56\n",
      "Processing video  56  -  60\n",
      "Processing video  60  -  64\n",
      "Processing video  64  -  68\n",
      "Processing video  68  -  72\n",
      "Processing video  72  -  76\n",
      "Processing video  76  -  80\n",
      "Processing video  80  -  84\n",
      "Processing video  84  -  88\n",
      "Processing video  88  -  92\n",
      "Processing video  92  -  96\n",
      "Processing video  96  -  100\n",
      "Processed  100  video\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'resnet_gavg'\n",
    "X_processed = extract_feature(X, resnet_gavg, 4)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d05b2",
   "metadata": {},
   "source": [
    "### VGG16_GAVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vgg16_gavg'\n",
    "X_processed = extract_feature(X, resnet_gavg, 4)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc2664",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b49c5abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of X:  (98, 40, 2048)\n",
      "Size of X:  32112776\n"
     ]
    }
   ],
   "source": [
    "print('Final shape of X: ', X_processed.shape)\n",
    "print('Size of X: ', sys.getsizeof(X_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "85b743b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to disk\n"
     ]
    }
   ],
   "source": [
    "np.save('F:/Kinetic_700/'+ model_name + '/' + category + '_processed', X_processed)\n",
    "np.save('F:/Kinetic_700/'+ model_name + '/' + category + '_label', y)\n",
    "\n",
    "print('Processed data saved to disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "418416bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 40, 2048)\n",
      "(98,)\n"
     ]
    }
   ],
   "source": [
    "X_reload = np.load('F:/Kinetic_700/'+ model_name + '/' + category + '_processed.npy')\n",
    "y_reload = np.load('F:/Kinetic_700/'+ model_name + '/' + category + '_label.npy')\n",
    "\n",
    "print(X_reload.shape)\n",
    "print(y_reload.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf465f5",
   "metadata": {},
   "source": [
    "### Free Ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9416aff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_processed, X, y, X_reload, y_reload\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac37681",
   "metadata": {},
   "source": [
    "## Split Train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b81ddc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1547"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "da5fbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_loader(classname, X_train, X_test, y_train, y_test, model_name):\n",
    "\n",
    "    if len(X_train) != 0 and len(y_train) != 0:\n",
    "        x_processed = np.load('f:/Kinetic_700/Train_data_processed/' + model_name + '/' + classname + '_processed.npy')\n",
    "        split = round(len(x_processed)*0.8)\n",
    "        X_train = np.concatenate((X_train, x_processed[0:split, :, :]))\n",
    "        X_test = np.concatenate((X_test, x_processed[split:len(x_processed), :, :]))\n",
    "\n",
    "        y_processed = np.load('f:/Kinetic_700/Train_data_processed/' + model_name + '/' + classname + '_label.npy')     \n",
    "        y_train = np.concatenate((y_train, y_processed[0:split]))\n",
    "        y_test = np.concatenate((y_test, y_processed[split:len(y_processed)]))\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, x_processed.shape, y_processed.shape, split\n",
    "    else:\n",
    "        x_processed = np.load('f:/Kinetic_700/Train_data_processed/' + model_name + '/' + classname + '_processed.npy')\n",
    "        split = round(len(x_processed)*0.8)\n",
    "        X_train = x_processed[0:split, :, :]\n",
    "        X_test = x_processed[split:len(x_processed), :, :]\n",
    "        \n",
    "        y_processed = np.load('f:/Kinetic_700/Train_data_processed/' + model_name + '/' + classname + '_label.npy')\n",
    "        y_train = y_processed[0:split]\n",
    "        y_test = y_processed[split:len(y_processed)]\n",
    "\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, x_processed.shape, y_processed.shape, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4213b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['dining', 'drawing', 'mopping_floor', 'reading_book', 'running_on_treadmill','sleeping','watching_tv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8fcfaae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of  dining  data set:  (1500, 40, 2048)\n",
      "Shape of  dining  label set:  (1500,)\n",
      "Shape of X_train:  (1200, 40, 2048)\n",
      "Shape of y_train:  (1200,)\n",
      "Shape of X_test:  (300, 40, 2048)\n",
      "Shape of y_test:  (300,) \n",
      "\n",
      "Shape of  drawing  data set:  (1476, 40, 2048)\n",
      "Shape of  drawing  label set:  (1476,)\n",
      "Shape of X_train:  (2381, 40, 2048)\n",
      "Shape of y_train:  (2381,)\n",
      "Shape of X_test:  (595, 40, 2048)\n",
      "Shape of y_test:  (595,) \n",
      "\n",
      "Shape of  mopping_floor  data set:  (1688, 40, 2048)\n",
      "Shape of  mopping_floor  label set:  (1688,)\n",
      "Shape of X_train:  (3731, 40, 2048)\n",
      "Shape of y_train:  (3731,)\n",
      "Shape of X_test:  (933, 40, 2048)\n",
      "Shape of y_test:  (933,) \n",
      "\n",
      "Shape of  reading_book  data set:  (1872, 40, 2048)\n",
      "Shape of  reading_book  label set:  (1872,)\n",
      "Shape of X_train:  (5229, 40, 2048)\n",
      "Shape of y_train:  (5229,)\n",
      "Shape of X_test:  (1307, 40, 2048)\n",
      "Shape of y_test:  (1307,) \n",
      "\n",
      "Shape of  running_on_treadmill  data set:  (1630, 40, 2048)\n",
      "Shape of  running_on_treadmill  label set:  (1630,)\n",
      "Shape of X_train:  (6533, 40, 2048)\n",
      "Shape of y_train:  (6533,)\n",
      "Shape of X_test:  (1633, 40, 2048)\n",
      "Shape of y_test:  (1633,) \n",
      "\n",
      "Shape of  sleeping  data set:  (1452, 40, 2048)\n",
      "Shape of  sleeping  label set:  (1452,)\n",
      "Shape of X_train:  (7695, 40, 2048)\n",
      "Shape of y_train:  (7695,)\n",
      "Shape of X_test:  (1923, 40, 2048)\n",
      "Shape of y_test:  (1923,) \n",
      "\n",
      "Shape of  watching_tv  data set:  (1342, 40, 2048)\n",
      "Shape of  watching_tv  label set:  (1342,)\n",
      "Shape of X_train:  (8769, 40, 2048)\n",
      "Shape of y_train:  (8769,)\n",
      "Shape of X_test:  (2191, 40, 2048)\n",
      "Shape of y_test:  (2191,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = [] \n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for classname in categories:\n",
    "    X_train, X_test, y_train, y_test, current_data_shape, current_label_shape, split = train_test_loader(classname, X_train, X_test, y_train, y_test, 'resnet_gavg')\n",
    "    \n",
    "    print(\"Shape of \", classname, \" data set: \", current_data_shape)\n",
    "    print(\"Shape of \", classname, \" label set: \", current_label_shape)\n",
    "    print(\"Shape of X_train: \", X_train.shape)\n",
    "    print(\"Shape of y_train: \", y_train.shape)\n",
    "    print(\"Shape of X_test: \", X_test.shape)\n",
    "    print(\"Shape of y_test: \", y_test.shape, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ce066fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train == 'dining'] = 0\n",
    "y_train[y_train == 'drawing'] = 1\n",
    "y_train[y_train == 'mopping_floor'] = 2\n",
    "y_train[y_train == 'reading_book'] = 3\n",
    "y_train[y_train == 'running_on_treadmill'] = 4\n",
    "y_train[y_train == 'sleeping'] = 5\n",
    "y_train[y_train == 'watching_tv'] = 6\n",
    "\n",
    "y_test[y_test == 'dining'] = 0\n",
    "y_test[y_test == 'drawing'] = 1\n",
    "y_test[y_test == 'mopping_floor'] = 2\n",
    "y_test[y_test == 'reading_book'] = 3\n",
    "y_test[y_test == 'running_on_treadmill'] = 4\n",
    "y_test[y_test == 'sleeping'] = 5\n",
    "y_test[y_test == 'watching_tv'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "794bbf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train:  (8769, 7)\n",
      "Shape of y_test:  (2191, 7)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)\n",
    "\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "30368d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train', X_train)\n",
    "np.save('y_train', y_train)\n",
    "np.save('X_test', X_test)\n",
    "np.save('y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c628f2",
   "metadata": {},
   "source": [
    "### To load manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77104cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for classname in CATEGORIES:\n",
    "    y, shape = load_processed_label(classname, y)\n",
    "    \n",
    "    print(\"Shape of \", classname, \" set: \", shape)\n",
    "    print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 'dining'] = 0\n",
    "y[y == 'drawing'] = 1\n",
    "y[y == 'mopping_floor'] = 2\n",
    "y[y == 'reading_book'] = 3\n",
    "y[y == 'running_on_treadmill'] = 4\n",
    "y[y == 'sleeping'] = 5\n",
    "y[y == 'watching_tv'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = to_categorical(y, num_classes=7)\n",
    "print(\"Shape of y_encoded: \", y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf077ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for classname in CATEGORIES:\n",
    "    X, shape = load_processed_data(classname, X)\n",
    "    \n",
    "    print(\"Shape of X: \", X.shape)\n",
    "    print(\"Shape of \", classname , ': ', shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8794cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fdfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train', X_train)\n",
    "np.save('y_train', y_train)\n",
    "np.save('X_test', X_test)\n",
    "np.save('y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfc571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5454ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f36705",
   "metadata": {},
   "source": [
    "### Load validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354adfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87425e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_eval = []\n",
    "y_eval = []\n",
    "\n",
    "for classname in CATEGORIES:\n",
    "    X_eval, X_current_shape = load_processed_data(classname,X_eval, 'resnet_gavg')\n",
    "    y_eval, y_current_shape = load_processed_label(classname,y_eval, 'resnet_gavg')\n",
    "    \n",
    "\n",
    "    print(\"Shape of \", classname, \" data set: \", X_current_shape)\n",
    "    print(\"Shape of \", classname, \" label set: \", y_current_shape)\n",
    "    print(\"Shape of X_eval: \", X_eval.shape)\n",
    "    print(\"Shape of y_eval: \", y_eval.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d44dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_eval', X_eval)\n",
    "np.save('y_eval', y_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
